import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import logging
import json
from os import listdir
from os.path import isfile, join
from collections import defaultdict

from gensim import corpora, models, similarities

logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

datadir="data/linkSO"
qtype='python'
lamda=10
beta=100
fields=['qHeader','qDescription','topVotedAnswer']
coeffs=[0.4,0.5,0.1]

class MyCorpus(object):
	def __init__(self,cpath,dictionary,documents=[],rep='bow'):
		self.rep='bow'
		self.cpath=cpath
		self.dict = dictionary
		if len(documents)>0:
			with open(cpath,'w') as f:
				for d in documents:
					f.write(d+"\n")

	def __iter__(self):
		for line in open(self.cpath):
			if self.rep=='bow':
				yield self.dictionary.doc2bow(d.split())

def prepare_corpus(datadir,qtype='python',recompute=False):
	pdf = pd.read_csv(join(datadir,"linkso/topublish/"+qtype+"/"+qtype+"_qid2all.txt"), sep ='\t', \
	                    names = ['qID', 'qHeader', 'qDescription', 'topVotedAnswer', 'type'])
	pdf['type']= qtype
	pdf.loc[:,'text']=pdf['qHeader'].astype(str)+pdf['qDescription'].astype(str)+pdf['topVotedAnswer'].astype(str)
	
	if isfile(join(datadir,qtype+"_qa.dict")) and (recompute==False):
		dictionary=corpora.Dictionary.load(join(datadir,qtype+"_qa.dict"))
		with open(join(datadir,qtype+"_pwC.json"),'r') as f:
			pwC=json.load(f)
		#corpus = MyCorpus(join(datadir,qtype+"_corpus.txt"),dictionary)
	else:


		documents = pdf['text']
		frequency = defaultdict(int)
		numtokens=0
		for d in documents:
		    for token in d.split():
		        frequency[token]+=1
		        numtokens+=1

		bowdocs = [d.split() for d in documents]
		dictionary = corpora.Dictionary(bowdocs)
		dictionary.save(join(datadir,qtype+"_qa.dict")) 

		#corpus = MyCorpus(join(datadir,qtype+"_corpus.txt"),dictionary,documents=documents)
		pwC={}
		for token in frequency:
			pwC[dictionary.token2id[token]]=frequency[token]/numtokens
		with open(join(datadir,qtype+"_pwC.json"),'w') as f:
			json.dump(pwC,f)

	return dictionary, pwC, pdf

def list2dict(l):
	dt={}
	for a,b in l:
		dt[a]=b
	return dt

def MRR(l):
	for i in range(len(l)):
		if l[i]==1:
			return 1/(i+1)
	return 0

def nDCG(l,n):
	sl = sorted(l,reverse=True)
	a,b=l[0],sl[0]
	for i in range(1,n):
		a+=l[i]/np.log2(i+1)
		b+=sl[i]/np.log2(i+1)
	return a/b

def LDA(dictionary,pdf,numtopics=10,recompute=False):

	if isfile(join(datadir,qtype+"_lda.model")) and (recompute==False):
		lda = models.LdaModel.load(join(datadir,qtype+"_lda.model"))
	else:

		documents = pdf['text']

		bowdocs = [dictionary.doc2bow(d.split()) for d in documents]
		lda = models.LdaModel(bowdocs, num_topics=10)
		lda.save(join(datadir,qtype+"_lda.model"))

	return lda

def topicsim(t1,t2):
	d1,d2=list2dict(t1),list2dict(t2)
	simscore=0
	for t in d1:
		simscore+=np.log(d1[t])+np.log(d2.get(t,0.001))
		return simscore

def main():
	dictionary, pwC, pdf = prepare_corpus("data/linkSO",recompute=False)
	lda=LDA(dictionary,pdf,numtopics=10)

	valids = pd.read_csv(join(datadir,"linkso/topublish/"+qtype+"/"+qtype+"_valid_qid.txt"), sep = '\t',\
	                      names = ['qId'])
	pyscore = pd.read_csv(join(datadir,"linkso/topublish/"+qtype+"/"+qtype+"_cosidf.txt"), sep ='\t', \
	                    names = ['qID_1', 'qID_2', 'score', 'label'], skiprows=1)
	valscore = pyscore.merge(valids,left_on='qID_1',right_on='qId',how='inner')
	print (valscore.columns)

	mrr, ndcg5, ndcg10 = 0,0,0
	numdocs=len(valscore)//30
	numdocs=100
	for i in range(0,numdocs*30,30):
		q = pdf.loc[pdf['qID']==valscore.loc[i]['qID_1']]
		qtopics = lda[dictionary.doc2bow(q['qDescription'].values[0].split())]
		counts_q = list2dict(dictionary.doc2bow(q['qDescription'].values[0].split()) )
		retrieval_list = []
		for j in range(30):
			qp = pdf.loc[pdf['qID']==valscore.loc[i+j]['qID_2']]
			dataqp,lenqp=[],0
			for f in range(len(fields)):
				doc=str(qp[fields[f]].values[0]).split()
				dataqp.append((coeffs[f],len(doc),list2dict(dictionary.doc2bow(doc))))
				lenqp+=dataqp[-1][1]
			score=0
			for w in counts_q:
				pwQA = sum([(x[0]*x[2].get(w,0))/x[1] for x in dataqp])
				psmooth = (lamda/(lenqp+lamda))*pwC[str(w)]+(lenqp/(lenqp+lamda))*pwQA
				score+=counts_q[w]*np.log(psmooth)
			qptopics = lda[dictionary.doc2bow(qp['text'].values[0].split())]
			#print ("score: {}, simscore: {}, label: {}".format(score,topicsim(qtopics,qptopics),valscore.loc[i+j]['label'] ))
			#score+=beta*topicsim(qtopics,qptopics)
			retrieval_list.append((score,valscore.loc[i+j]['label']))
		retrieval_list = sorted(retrieval_list,key= lambda x: x[0],reverse=True)
		relevance_list = [x[1] for x in retrieval_list]
		mrr+=MRR(relevance_list) 
		ndcg5+=nDCG(relevance_list,5)
		ndcg10+=nDCG(relevance_list,10)
		print ("{}/{}".format(i/30,numdocs))
		#print ("-------------------------------------")
	mrr/=(numdocs)
	ndcg5/=(numdocs)
	ndcg10/=(numdocs)
	print ("scores-> mrr={}, ndcg5={}, ndcg10={}".format(mrr,ndcg5,ndcg10))

if __name__ == '__main__':
	main()